{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QipCL4N-745s",
        "outputId": "3279e58f-da77-4f60-905a-8c9004fa2398"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Using Colab cache for faster access to the 'person-segmentation' dataset.\n",
            "Using Colab cache for faster access to the 'supervisely-filtered-segmentation-person-dataset' dataset.\n",
            "Data source import complete.\n",
            "/kaggle/input/person-segmentation\n",
            "/kaggle/input/supervisely-filtered-segmentation-person-dataset\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import kagglehub\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.models import mobilenet_v3_large, MobileNet_V3_Large_Weights\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "nikhilroxtomar_person_segmentation_path = kagglehub.dataset_download('nikhilroxtomar/person-segmentation')\n",
        "tapakah68_supervisely_filtered_segmentation_person_dataset_path = kagglehub.dataset_download('tapakah68/supervisely-filtered-segmentation-person-dataset')\n",
        "\n",
        "print('Data source import complete.')\n",
        "print(nikhilroxtomar_person_segmentation_path)\n",
        "print(tapakah68_supervisely_filtered_segmentation_person_dataset_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames[:1]:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXBk-4Co6Had",
        "outputId": "73cd81df-b091-4fef-c68e-2058ccae1d32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/kaggle/input/person-segmentation/people_segmentation/README\n",
            "/kaggle/input/person-segmentation/people_segmentation/segmentation/val.txt\n",
            "/kaggle/input/person-segmentation/people_segmentation/images/pexels-photo-219004.jpg\n",
            "/kaggle/input/person-segmentation/people_segmentation/masks/girl-beautiful-beautiful-girl-face.png\n",
            "/kaggle/input/supervisely-filtered-segmentation-person-dataset/df.csv\n",
            "/kaggle/input/supervisely-filtered-segmentation-person-dataset/supervisely_person_clean_2667_img/supervisely_person_clean_2667_img/collage/ds6_pexels-photo-792385.jpg\n",
            "/kaggle/input/supervisely-filtered-segmentation-person-dataset/supervisely_person_clean_2667_img/supervisely_person_clean_2667_img/images/ds8_pexels-photo-838817.png\n",
            "/kaggle/input/supervisely-filtered-segmentation-person-dataset/supervisely_person_clean_2667_img/supervisely_person_clean_2667_img/masks/ds8_pexels-photo-838817.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "images_path =  \"/kaggle/input/supervisely-filtered-segmentation-person-dataset/supervisely_person_clean_2667_img/supervisely_person_clean_2667_img/\"\n",
        "masks_path  = \"/kaggle/input/supervisely-filtered-segmentation-person-dataset/supervisely_person_clean_2667_img/supervisely_person_clean_2667_img/\"\n",
        "\n",
        "images_path2=\"/kaggle/input/person-segmentation/people_segmentation/images/\"\n",
        "masks_path2=\"/kaggle/input/person-segmentation/people_segmentation/masks/\"\n",
        "df =  pd.read_csv('/kaggle/input/supervisely-filtered-segmentation-person-dataset/df.csv')\n",
        "df.head()\n",
        "images_path2_list = sorted(os.listdir(images_path2))\n",
        "masks_path2_list = sorted(os.listdir(masks_path2))\n",
        "df2 = df[[\"images\", \"masks\"]].copy()\n",
        "df2[\"images\"] = df2['images'].apply(lambda x: images_path + x)\n",
        "df2[\"masks\"]  = df2['masks'].apply(lambda x: masks_path + x)\n",
        "df2[\"coef\"]   = 1\n",
        "\n",
        "df3 = pd.DataFrame({\n",
        "    \"images\": [images_path2 + elt for elt in images_path2_list],\n",
        "    \"masks\":  [masks_path2 + elt for elt in masks_path2_list],\n",
        "    \"coef\":   255\n",
        "})\n",
        "\n",
        "final_df = pd.concat([df2, df3], ignore_index=True)\n",
        "\n",
        "X_train_raw, X_test_raw  =  train_test_split(final_df, test_size=0.1, random_state=42)"
      ],
      "metadata": {
        "id": "b36mwrGe-2Hz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df = final_df.copy()\n",
        "final_df[\"source\"] = np.where(final_df[\"coef\"] == 1, \"supervisely\", \"person_seg\")\n",
        "print(\"Tổng số mẫu:\", len(final_df))\n",
        "print(\"\\nSố mẫu theo nguồn:\")\n",
        "print(final_df[\"source\"].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3EnWEsYb81BC",
        "outputId": "15ae9f10-e7ea-4a41-d8f9-b03d2242e81e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tổng số mẫu: 8345\n",
            "\n",
            "Số mẫu theo nguồn:\n",
            "source\n",
            "person_seg     5678\n",
            "supervisely    2667\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def dice_loss(logits, target, eps=1e-6):\n",
        "    pred = torch.sigmoid(logits)\n",
        "    target = target.float()\n",
        "    intersection = (pred * target).sum()\n",
        "    return 1 - (2. * intersection + eps) / (pred.sum() + target.sum() + eps)\n",
        "\n",
        "def dice_score_from_logits(logits, target, thr=0.5, eps=1e-6):\n",
        "    prob = torch.sigmoid(logits)\n",
        "    pred = (prob > thr).float()\n",
        "    inter = (pred * target).sum()\n",
        "    union = pred.sum() + target.sum()\n",
        "    return ((2*inter + eps) / (union + eps)).item()\n",
        "\n",
        "def logits_to_probs_preds(logits, thr=0.5):\n",
        "    probs = torch.sigmoid(logits)\n",
        "    preds = (probs > thr).float()\n",
        "    return probs, preds\n",
        "\n",
        "def batch_pixel_accuracy(preds, targets):\n",
        "    correct = (preds == targets).float().sum()\n",
        "    total = torch.numel(targets)\n",
        "    return (correct / total).item()\n",
        "\n",
        "def batch_iou(preds, targets, eps=1e-6):\n",
        "    inter = (preds * targets).sum(dim=(1,2,3))\n",
        "    union = (preds + targets - preds*targets).sum(dim=(1,2,3))\n",
        "    iou_per_image = ((inter + eps) / (union + eps))\n",
        "    return iou_per_image.mean().item()\n",
        "\n",
        "def batch_dice(preds, targets, eps=1e-6):\n",
        "    inter = (preds * targets).sum(dim=(1,2,3))\n",
        "    denom = preds.sum(dim=(1,2,3)) + targets.sum(dim=(1,2,3))\n",
        "    dice_per_image = ((2*inter + eps) / (denom + eps))\n",
        "    return dice_per_image.mean().item()\n",
        "\n",
        "def batch_precision_recall_f1(preds, targets, eps=1e-6):\n",
        "    tp = (preds * targets).sum(dim=(1,2,3))\n",
        "    fp = (preds * (1 - targets)).sum(dim=(1,2,3))\n",
        "    fn = ((1 - preds) * targets).sum(dim=(1,2,3))\n",
        "\n",
        "    precision = (tp + eps) / (tp + fp + eps)\n",
        "    recall = (tp + eps) / (tp + fn + eps)\n",
        "    f1 = (2 * precision * recall + eps) / (precision + recall + eps)\n",
        "\n",
        "    return precision.mean().item(), recall.mean().item(), f1.mean().item()\n"
      ],
      "metadata": {
        "id": "82Fu2vwtGg59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#RAW TRAIN\n",
        "class RawSegDataset(Dataset):\n",
        "    def __init__(self, df, size=512):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.size = size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.loc[idx]\n",
        "        img_path  = row[\"images\"]\n",
        "        mask_path = row[\"masks\"]\n",
        "        coef      = row.get(\"coef\", 1)\n",
        "\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        mask = Image.open(mask_path).convert(\"L\")\n",
        "\n",
        "        img = img.resize((self.size, self.size), Image.BILINEAR)\n",
        "        mask = mask.resize((self.size, self.size), Image.NEAREST)\n",
        "\n",
        "        # --- Convert to numpy ---\n",
        "        img_np = np.array(img, dtype=np.float32)        # 0..255\n",
        "        mask_np = np.array(mask, dtype=np.float32)      # 0..255 (likely)\n",
        "\n",
        "        # --- Robust normalization for mask ---\n",
        "        # Normalize mask to [0,1] regardless of original encoding:\n",
        "        max_val = mask_np.max() if mask_np.max() > 0 else 1.0\n",
        "        if max_val > 1.0:\n",
        "            mask_np = mask_np / max_val   # now in [0,1]\n",
        "        mask_bin = (mask_np >= 0.5).astype(np.float32)\n",
        "\n",
        "        # --- Image scaling + ImageNet normalization ---\n",
        "        img_np = img_np / 255.0   # to [0,1]\n",
        "\n",
        "        # to tensor (C,H,W)\n",
        "        img_t = torch.from_numpy(img_np).permute(2,0,1).float()\n",
        "        mask_t = torch.from_numpy(mask_bin).unsqueeze(0).float()\n",
        "\n",
        "        # ImageNet normalize\n",
        "        mean = torch.tensor([0.485, 0.456, 0.406], dtype=torch.float32).view(3,1,1)\n",
        "        std  = torch.tensor([0.229, 0.224, 0.225], dtype=torch.float32).view(3,1,1)\n",
        "        img_t = (img_t - mean) / std\n",
        "\n",
        "        return img_t, mask_t\n",
        "\n",
        "class ASPP(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, 1)\n",
        "        self.conv6 = nn.Conv2d(in_channels, out_channels, 3, padding=6, dilation=6)\n",
        "        self.conv12 = nn.Conv2d(in_channels, out_channels, 3, padding=12, dilation=12)\n",
        "        self.conv18 = nn.Conv2d(in_channels, out_channels, 3, padding=18, dilation=18)\n",
        "\n",
        "        self.global_pool = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Conv2d(in_channels, out_channels, 1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        self.out_conv = nn.Conv2d(out_channels * 5, out_channels, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h, w = x.shape[2], x.shape[3]\n",
        "        y1 = self.conv1(x)\n",
        "        y2 = self.conv6(x)\n",
        "        y3 = self.conv12(x)\n",
        "        y4 = self.conv18(x)\n",
        "        y5 = self.global_pool(x)\n",
        "        y5 = F.interpolate(y5, size=(h, w), mode=\"bilinear\", align_corners=False)\n",
        "        y = torch.cat([y1, y2, y3, y4, y5], dim=1)\n",
        "        return self.out_conv(y)\n",
        "\n",
        "class DeepLabV3_MobileNetV3Large(nn.Module):\n",
        "    def __init__(self, num_classes=1):\n",
        "        super().__init__()\n",
        "        base = mobilenet_v3_large(weights=MobileNet_V3_Large_Weights.DEFAULT)\n",
        "        self.backbone = base.features\n",
        "        backbone_out = 960\n",
        "        self.aspp = ASPP(backbone_out, 256)\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Conv2d(256, 128, 3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, num_classes, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.backbone(x)\n",
        "        x = self.aspp(x)\n",
        "        x = self.decoder(x)\n",
        "        x = F.interpolate(x, size=(512, 512), mode=\"bilinear\", align_corners=False)\n",
        "        return x\n",
        "\n",
        "train_ds = RawSegDataset(X_train_raw, size=512)\n",
        "val_ds   = RawSegDataset(X_test_raw,  size=512)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=8, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=8, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = DeepLabV3_MobileNetV3Large(num_classes=1).to(device)\n",
        "\n",
        "criterion_bce = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "num_epochs = 20\n",
        "history = {\n",
        "    \"train_loss\": [],\n",
        "    \"val_loss\": [],\n",
        "    \"val_iou\": [],\n",
        "    \"val_dice\": [],\n",
        "    \"val_acc\": [],\n",
        "    \"val_prec\": [],\n",
        "    \"val_recall\": [],\n",
        "    \"val_f1\": []\n",
        "}\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    train_iou = 0.0\n",
        "    train_dice = 0.0\n",
        "    train_acc = 0.0\n",
        "    train_steps = 0\n",
        "\n",
        "    for imgs, masks in tqdm(train_loader, desc=f\"Train epoch {epoch+1}\"):\n",
        "        imgs = imgs.to(device).float()\n",
        "        masks = masks.to(device).float()\n",
        "\n",
        "        logits = model(imgs)\n",
        "        bce = criterion_bce(logits, masks)\n",
        "        dsc = dice_loss(logits, masks)\n",
        "        loss = bce + dsc\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            _, preds = logits_to_probs_preds(logits, thr=0.5)\n",
        "            train_iou += batch_iou(preds, masks)\n",
        "            train_dice += batch_dice(preds, masks)\n",
        "            train_acc += batch_pixel_accuracy(preds, masks)\n",
        "            train_steps += 1\n",
        "\n",
        "    avg_loss = running_loss / max(1, len(train_loader))\n",
        "    avg_train_iou = train_iou / max(1, train_steps)\n",
        "    avg_train_dice = train_dice / max(1, train_steps)\n",
        "    avg_train_acc = train_acc / max(1, train_steps)\n",
        "    print(f\"Epoch {epoch+1} - train avg loss: {avg_loss:.4f} | IoU: {avg_train_iou:.4f} | Dice: {avg_train_dice:.4f} | Acc: {avg_train_acc:.4f}\")\n",
        "    history[\"train_loss\"].append(avg_loss)\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_steps = 0\n",
        "\n",
        "    sum_iou = 0.0\n",
        "    sum_dice = 0.0\n",
        "    sum_acc = 0.0\n",
        "    sum_prec = 0.0\n",
        "    sum_recall = 0.0\n",
        "    sum_f1 = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, masks in val_loader:\n",
        "            imgs = imgs.to(device).float()\n",
        "            masks = masks.to(device).float()\n",
        "\n",
        "            logits = model(imgs)\n",
        "            bce = criterion_bce(logits, masks)\n",
        "            dsc = dice_loss(logits, masks)\n",
        "            batch_loss = (bce + dsc).item()\n",
        "            val_loss += batch_loss\n",
        "\n",
        "            probs, preds = logits_to_probs_preds(logits, thr=0.5)\n",
        "\n",
        "            # metrics\n",
        "            iou_b = batch_iou(preds, masks)\n",
        "            dice_b = batch_dice(preds, masks)\n",
        "            acc_b = batch_pixel_accuracy(preds, masks)\n",
        "            prec_b, recall_b, f1_b = batch_precision_recall_f1(preds, masks)\n",
        "\n",
        "            sum_iou += iou_b\n",
        "            sum_dice += dice_b\n",
        "            sum_acc += acc_b\n",
        "            sum_prec += prec_b\n",
        "            sum_recall += recall_b\n",
        "            sum_f1 += f1_b\n",
        "\n",
        "            val_steps += 1\n",
        "\n",
        "    avg_val_loss = val_loss / max(1, val_steps)\n",
        "    avg_val_iou = sum_iou / max(1, val_steps)\n",
        "    avg_val_dice = sum_dice / max(1, val_steps)\n",
        "    avg_val_acc = sum_acc / max(1, val_steps)\n",
        "    avg_val_prec = sum_prec / max(1, val_steps)\n",
        "    avg_val_recall = sum_recall / max(1, val_steps)\n",
        "    avg_val_f1 = sum_f1 / max(1, val_steps)\n",
        "\n",
        "    history[\"val_loss\"].append(avg_val_loss)\n",
        "    history[\"val_iou\"].append(avg_val_iou)\n",
        "    history[\"val_dice\"].append(avg_val_dice)\n",
        "    history[\"val_acc\"].append(avg_val_acc)\n",
        "    history[\"val_prec\"].append(avg_val_prec)\n",
        "    history[\"val_recall\"].append(avg_val_recall)\n",
        "    history[\"val_f1\"].append(avg_val_f1)\n",
        "\n",
        "    print(f\"Val loss: {avg_val_loss:.4f} | IoU: {avg_val_iou:.4f} | Dice: {avg_val_dice:.4f} | Acc: {avg_val_acc:.4f}\")\n",
        "    print(f\"Precision: {avg_val_prec:.4f} | Recall: {avg_val_recall:.4f} | F1: {avg_val_f1:.4f}\")\n",
        "\n",
        "import pandas as pd\n",
        "hist_df = pd.DataFrame(history)\n",
        "hist_df.to_csv(\"/content/drive/MyDrive/Data Mining/Project/Model/training_history_raw_metrics.csv\", index=False)\n",
        "print(\"Saved training history to Drive (training_history_raw_metrics.csv)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kt-QBOohArfP",
        "outputId": "67ee82b1-e6d4-400c-83cc-b62d4af1fb73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/mobilenet_v3_large-5c1a4163.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v3_large-5c1a4163.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 21.1M/21.1M [00:00<00:00, 182MB/s]\n",
            "Train epoch 1: 100%|██████████| 939/939 [05:18<00:00,  2.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 - train avg loss: 0.2607 | IoU: 0.7841 | Dice: 0.8644 | Acc: 0.9484\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss: 0.1726 | IoU: 0.8361 | Dice: 0.9022 | Acc: 0.9669\n",
            "Precision: 0.9166 | Recall: 0.8982 | F1: 0.9022\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train epoch 2: 100%|██████████| 939/939 [05:00<00:00,  3.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 - train avg loss: 0.1618 | IoU: 0.8419 | Dice: 0.9053 | Acc: 0.9681\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss: 0.1496 | IoU: 0.8517 | Dice: 0.9119 | Acc: 0.9708\n",
            "Precision: 0.9162 | Recall: 0.9154 | F1: 0.9130\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train epoch 3: 100%|██████████| 939/939 [04:51<00:00,  3.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 - train avg loss: 0.1342 | IoU: 0.8585 | Dice: 0.9160 | Acc: 0.9737\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss: 0.1377 | IoU: 0.8592 | Dice: 0.9159 | Acc: 0.9731\n",
            "Precision: 0.9190 | Recall: 0.9221 | F1: 0.9183\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train epoch 4: 100%|██████████| 939/939 [04:48<00:00,  3.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 - train avg loss: 0.1163 | IoU: 0.8713 | Dice: 0.9242 | Acc: 0.9772\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss: 0.1436 | IoU: 0.8613 | Dice: 0.9171 | Acc: 0.9730\n",
            "Precision: 0.9257 | Recall: 0.9181 | F1: 0.9194\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train epoch 5: 100%|██████████| 939/939 [04:47<00:00,  3.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 - train avg loss: 0.1060 | IoU: 0.8780 | Dice: 0.9286 | Acc: 0.9792\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss: 0.1300 | IoU: 0.8692 | Dice: 0.9223 | Acc: 0.9759\n",
            "Precision: 0.9245 | Recall: 0.9272 | F1: 0.9247\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train epoch 6: 100%|██████████| 939/939 [04:49<00:00,  3.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 - train avg loss: 0.0969 | IoU: 0.8847 | Dice: 0.9328 | Acc: 0.9810\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss: 0.1333 | IoU: 0.8707 | Dice: 0.9233 | Acc: 0.9751\n",
            "Precision: 0.9200 | Recall: 0.9327 | F1: 0.9279\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train epoch 7: 100%|██████████| 939/939 [04:49<00:00,  3.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 - train avg loss: 0.0938 | IoU: 0.8873 | Dice: 0.9344 | Acc: 0.9816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss: 0.1352 | IoU: 0.8729 | Dice: 0.9245 | Acc: 0.9761\n",
            "Precision: 0.9239 | Recall: 0.9311 | F1: 0.9269\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train epoch 8: 100%|██████████| 939/939 [04:51<00:00,  3.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 - train avg loss: 0.0902 | IoU: 0.8904 | Dice: 0.9366 | Acc: 0.9823\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss: 0.1267 | IoU: 0.8754 | Dice: 0.9255 | Acc: 0.9774\n",
            "Precision: 0.9339 | Recall: 0.9270 | F1: 0.9278\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train epoch 9: 100%|██████████| 939/939 [04:51<00:00,  3.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 - train avg loss: 0.0892 | IoU: 0.8920 | Dice: 0.9374 | Acc: 0.9825\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss: 0.1297 | IoU: 0.8764 | Dice: 0.9264 | Acc: 0.9771\n",
            "Precision: 0.9259 | Recall: 0.9366 | F1: 0.9286\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train epoch 10: 100%|██████████| 939/939 [04:48<00:00,  3.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 - train avg loss: 0.0818 | IoU: 0.8969 | Dice: 0.9404 | Acc: 0.9839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss: 0.1227 | IoU: 0.8798 | Dice: 0.9286 | Acc: 0.9781\n",
            "Precision: 0.9298 | Recall: 0.9327 | F1: 0.9331\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train epoch 11: 100%|██████████| 939/939 [04:49<00:00,  3.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11 - train avg loss: 0.0810 | IoU: 0.8979 | Dice: 0.9410 | Acc: 0.9840\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss: 0.1290 | IoU: 0.8759 | Dice: 0.9259 | Acc: 0.9769\n",
            "Precision: 0.9266 | Recall: 0.9316 | F1: 0.9292\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train epoch 12: 100%|██████████| 939/939 [04:52<00:00,  3.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12 - train avg loss: 0.0762 | IoU: 0.9015 | Dice: 0.9432 | Acc: 0.9850\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss: 0.1248 | IoU: 0.8820 | Dice: 0.9305 | Acc: 0.9783\n",
            "Precision: 0.9258 | Recall: 0.9400 | F1: 0.9329\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train epoch 13: 100%|██████████| 939/939 [05:03<00:00,  3.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13 - train avg loss: 0.0762 | IoU: 0.9023 | Dice: 0.9440 | Acc: 0.9850\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss: 0.1236 | IoU: 0.8780 | Dice: 0.9270 | Acc: 0.9780\n",
            "Precision: 0.9302 | Recall: 0.9299 | F1: 0.9317\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train epoch 14: 100%|██████████| 939/939 [04:59<00:00,  3.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14 - train avg loss: 0.0772 | IoU: 0.9021 | Dice: 0.9438 | Acc: 0.9849\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss: 0.1222 | IoU: 0.8814 | Dice: 0.9293 | Acc: 0.9786\n",
            "Precision: 0.9344 | Recall: 0.9325 | F1: 0.9325\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train epoch 15:   3%|▎         | 25/939 [00:08<04:43,  3.22it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = \"/content/drive/MyDrive/Data Mining/Project/Model/deeplabv3_mbv3_raw.pth\"\n",
        "\n",
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "}, save_path)\n",
        "\n",
        "print(f\"Model saved to {save_path}\")"
      ],
      "metadata": {
        "id": "rbxfSJU9DZyg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}