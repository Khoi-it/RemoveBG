{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUx8GM6cBw6_",
        "outputId": "63672324-af44-46fd-c8a3-0d42577c0f63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Using Colab cache for faster access to the 'person-segmentation' dataset.\n",
            "Using Colab cache for faster access to the 'supervisely-filtered-segmentation-person-dataset' dataset.\n",
            "Data source import complete.\n",
            "/kaggle/input/person-segmentation\n",
            "/kaggle/input/supervisely-filtered-segmentation-person-dataset\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import kagglehub\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image, ImageOps\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.models import mobilenet_v3_large, MobileNet_V3_Large_Weights\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import torchvision.transforms as T\n",
        "import torchvision.transforms.functional as TF\n",
        "from torchvision.transforms.functional import InterpolationMode\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "nikhilroxtomar_person_segmentation_path = kagglehub.dataset_download('nikhilroxtomar/person-segmentation')\n",
        "tapakah68_supervisely_filtered_segmentation_person_dataset_path = kagglehub.dataset_download('tapakah68/supervisely-filtered-segmentation-person-dataset')\n",
        "\n",
        "print('Data source import complete.')\n",
        "print(nikhilroxtomar_person_segmentation_path)\n",
        "print(tapakah68_supervisely_filtered_segmentation_person_dataset_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFKTGQ8iGY86"
      },
      "outputs": [],
      "source": [
        "images_path =  \"/kaggle/input/supervisely-filtered-segmentation-person-dataset/supervisely_person_clean_2667_img/supervisely_person_clean_2667_img/\"\n",
        "masks_path  = \"/kaggle/input/supervisely-filtered-segmentation-person-dataset/supervisely_person_clean_2667_img/supervisely_person_clean_2667_img/\"\n",
        "\n",
        "images_path2=\"/kaggle/input/person-segmentation/people_segmentation/images/\"\n",
        "masks_path2=\"/kaggle/input/person-segmentation/people_segmentation/masks/\"\n",
        "df =  pd.read_csv('/kaggle/input/supervisely-filtered-segmentation-person-dataset/df.csv')\n",
        "df.head()\n",
        "images_path2_list = sorted(os.listdir(images_path2))\n",
        "masks_path2_list = sorted(os.listdir(masks_path2))\n",
        "df2 = df[[\"images\", \"masks\"]].copy()\n",
        "df2[\"images\"] = df2['images'].apply(lambda x: images_path + x)\n",
        "df2[\"masks\"]  = df2['masks'].apply(lambda x: masks_path + x)\n",
        "df2[\"coef\"]   = 1\n",
        "\n",
        "df3 = pd.DataFrame({\n",
        "    \"images\": [images_path2 + elt for elt in images_path2_list],\n",
        "    \"masks\":  [masks_path2 + elt for elt in masks_path2_list],\n",
        "    \"coef\":   255\n",
        "})\n",
        "\n",
        "final_df = pd.concat([df2, df3], ignore_index=True)\n",
        "\n",
        "X_train_raw, X_test_raw  =  train_test_split(final_df, test_size=0.1, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sls3PHwDvbg2",
        "outputId": "76848c13-13d0-47fe-fe5e-9538c83ee979"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tổng số mẫu: 8345\n",
            "\n",
            "Số mẫu theo nguồn:\n",
            "source\n",
            "person_seg     5678\n",
            "supervisely    2667\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "final_df = final_df.copy()\n",
        "final_df[\"source\"] = np.where(final_df[\"coef\"] == 1, \"supervisely\", \"person_seg\")\n",
        "print(\"Tổng số mẫu:\", len(final_df))\n",
        "print(\"\\nSố mẫu theo nguồn:\")\n",
        "print(final_df[\"source\"].value_counts())\n",
        "\n",
        "def dice_loss(logits, target, eps=1e-6):\n",
        "    pred = torch.sigmoid(logits)\n",
        "    target = target.float()\n",
        "    intersection = (pred * target).sum()\n",
        "    return 1 - (2. * intersection + eps) / (pred.sum() + target.sum() + eps)\n",
        "\n",
        "def dice_score_from_logits(logits, target, thr=0.5, eps=1e-6):\n",
        "    prob = torch.sigmoid(logits)\n",
        "    pred = (prob > thr).float()\n",
        "    inter = (pred * target).sum()\n",
        "    union = pred.sum() + target.sum()\n",
        "    return ((2*inter + eps) / (union + eps)).item()\n",
        "\n",
        "def logits_to_probs_preds(logits, thr=0.5):\n",
        "    probs = torch.sigmoid(logits)\n",
        "    preds = (probs > thr).float()\n",
        "    return probs, preds\n",
        "\n",
        "def batch_pixel_accuracy(preds, targets):\n",
        "    correct = (preds == targets).float().sum()\n",
        "    total = torch.numel(targets)\n",
        "    return (correct / total).item()\n",
        "\n",
        "def batch_iou(preds, targets, eps=1e-6):\n",
        "    inter = (preds * targets).sum(dim=(1,2,3))\n",
        "    union = (preds + targets - preds*targets).sum(dim=(1,2,3))\n",
        "    iou_per_image = ((inter + eps) / (union + eps))\n",
        "    return iou_per_image.mean().item()\n",
        "\n",
        "def batch_dice(preds, targets, eps=1e-6):\n",
        "    inter = (preds * targets).sum(dim=(1,2,3))\n",
        "    denom = preds.sum(dim=(1,2,3)) + targets.sum(dim=(1,2,3))\n",
        "    dice_per_image = ((2*inter + eps) / (denom + eps))\n",
        "    return dice_per_image.mean().item()\n",
        "\n",
        "def batch_precision_recall_f1(preds, targets, eps=1e-6):\n",
        "    tp = (preds * targets).sum(dim=(1,2,3))\n",
        "    fp = (preds * (1 - targets)).sum(dim=(1,2,3))\n",
        "    fn = ((1 - preds) * targets).sum(dim=(1,2,3))\n",
        "\n",
        "    precision = (tp + eps) / (tp + fp + eps)\n",
        "    recall = (tp + eps) / (tp + fn + eps)\n",
        "    f1 = (2 * precision * recall + eps) / (precision + recall + eps)\n",
        "\n",
        "    return precision.mean().item(), recall.mean().item(), f1.mean().item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "lkj36cNatkiB",
        "outputId": "f5fcd2ac-f215-46de-df49-254357687556"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train epoch 1: 100%|██████████| 939/939 [07:50<00:00,  2.00it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 - train avg loss: 0.2739 | IoU: 0.7788 | Dice: 0.8614 | Acc: 0.9443\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val loss: 0.1785 | IoU: 0.8333 | Dice: 0.8997 | Acc: 0.9650\n",
            "Precision: 0.8967 | Recall: 0.9155 | F1: 0.9020\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train epoch 2: 100%|██████████| 939/939 [07:50<00:00,  2.00it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 - train avg loss: 0.1759 | IoU: 0.8359 | Dice: 0.9017 | Acc: 0.9648\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val loss: 0.1519 | IoU: 0.8488 | Dice: 0.9099 | Acc: 0.9703\n",
            "Precision: 0.9186 | Recall: 0.9114 | F1: 0.9099\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train epoch 3: 100%|██████████| 939/939 [07:47<00:00,  2.01it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 - train avg loss: 0.1526 | IoU: 0.8500 | Dice: 0.9108 | Acc: 0.9697\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val loss: 0.1440 | IoU: 0.8548 | Dice: 0.9126 | Acc: 0.9722\n",
            "Precision: 0.9256 | Recall: 0.9121 | F1: 0.9138\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train epoch 4: 100%|██████████| 939/939 [07:52<00:00,  1.99it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 - train avg loss: 0.1367 | IoU: 0.8595 | Dice: 0.9169 | Acc: 0.9729\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val loss: 0.1385 | IoU: 0.8637 | Dice: 0.9188 | Acc: 0.9732\n",
            "Precision: 0.9064 | Recall: 0.9402 | F1: 0.9224\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train epoch 5: 100%|██████████| 939/939 [07:56<00:00,  1.97it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5 - train avg loss: 0.1261 | IoU: 0.8666 | Dice: 0.9214 | Acc: 0.9750\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val loss: 0.1330 | IoU: 0.8673 | Dice: 0.9205 | Acc: 0.9749\n",
            "Precision: 0.9128 | Recall: 0.9350 | F1: 0.9252\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train epoch 6: 100%|██████████| 939/939 [07:54<00:00,  1.98it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6 - train avg loss: 0.1184 | IoU: 0.8724 | Dice: 0.9253 | Acc: 0.9766\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val loss: 0.1423 | IoU: 0.8627 | Dice: 0.9177 | Acc: 0.9733\n",
            "Precision: 0.9200 | Recall: 0.9233 | F1: 0.9201\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train epoch 7: 100%|██████████| 939/939 [07:57<00:00,  1.97it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7 - train avg loss: 0.1123 | IoU: 0.8766 | Dice: 0.9280 | Acc: 0.9778\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val loss: 0.1303 | IoU: 0.8666 | Dice: 0.9196 | Acc: 0.9755\n",
            "Precision: 0.9380 | Recall: 0.9137 | F1: 0.9208\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train epoch 8: 100%|██████████| 939/939 [07:51<00:00,  1.99it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8 - train avg loss: 0.1076 | IoU: 0.8790 | Dice: 0.9294 | Acc: 0.9786\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val loss: 0.1251 | IoU: 0.8709 | Dice: 0.9230 | Acc: 0.9764\n",
            "Precision: 0.9333 | Recall: 0.9231 | F1: 0.9253\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train epoch 9: 100%|██████████| 939/939 [07:52<00:00,  1.99it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9 - train avg loss: 0.1034 | IoU: 0.8824 | Dice: 0.9316 | Acc: 0.9795\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val loss: 0.1225 | IoU: 0.8741 | Dice: 0.9250 | Acc: 0.9771\n",
            "Precision: 0.9340 | Recall: 0.9253 | F1: 0.9273\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train epoch 10: 100%|██████████| 939/939 [07:52<00:00,  1.99it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10 - train avg loss: 0.1032 | IoU: 0.8837 | Dice: 0.9324 | Acc: 0.9796\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val loss: 0.1286 | IoU: 0.8674 | Dice: 0.9196 | Acc: 0.9760\n",
            "Precision: 0.9375 | Recall: 0.9136 | F1: 0.9219\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train epoch 11: 100%|██████████| 939/939 [07:46<00:00,  2.01it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11 - train avg loss: 0.0994 | IoU: 0.8865 | Dice: 0.9344 | Acc: 0.9802\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val loss: 0.1207 | IoU: 0.8769 | Dice: 0.9267 | Acc: 0.9775\n",
            "Precision: 0.9288 | Recall: 0.9326 | F1: 0.9279\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train epoch 12: 100%|██████████| 939/939 [07:48<00:00,  2.00it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12 - train avg loss: 0.0931 | IoU: 0.8899 | Dice: 0.9364 | Acc: 0.9814\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val loss: 0.1182 | IoU: 0.8789 | Dice: 0.9272 | Acc: 0.9780\n",
            "Precision: 0.9228 | Recall: 0.9412 | F1: 0.9292\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train epoch 13: 100%|██████████| 939/939 [07:48<00:00,  2.00it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13 - train avg loss: 0.0945 | IoU: 0.8896 | Dice: 0.9362 | Acc: 0.9812\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val loss: 0.1182 | IoU: 0.8796 | Dice: 0.9285 | Acc: 0.9782\n",
            "Precision: 0.9290 | Recall: 0.9361 | F1: 0.9320\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train epoch 14: 100%|██████████| 939/939 [07:57<00:00,  1.97it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 14 - train avg loss: 0.0925 | IoU: 0.8917 | Dice: 0.9377 | Acc: 0.9816\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val loss: 0.1205 | IoU: 0.8775 | Dice: 0.9271 | Acc: 0.9778\n",
            "Precision: 0.9361 | Recall: 0.9247 | F1: 0.9316\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train epoch 15: 100%|██████████| 939/939 [07:57<00:00,  1.97it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15 - train avg loss: 0.0891 | IoU: 0.8937 | Dice: 0.9387 | Acc: 0.9823\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val loss: 0.1156 | IoU: 0.8817 | Dice: 0.9301 | Acc: 0.9789\n",
            "Precision: 0.9272 | Recall: 0.9391 | F1: 0.9322\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train epoch 16: 100%|██████████| 939/939 [07:56<00:00,  1.97it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 16 - train avg loss: 0.0868 | IoU: 0.8955 | Dice: 0.9400 | Acc: 0.9827\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val loss: 0.1141 | IoU: 0.8808 | Dice: 0.9285 | Acc: 0.9794\n",
            "Precision: 0.9375 | Recall: 0.9280 | F1: 0.9320\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train epoch 17: 100%|██████████| 939/939 [07:49<00:00,  2.00it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 17 - train avg loss: 0.0856 | IoU: 0.8963 | Dice: 0.9404 | Acc: 0.9829\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val loss: 0.1222 | IoU: 0.8804 | Dice: 0.9287 | Acc: 0.9780\n",
            "Precision: 0.9227 | Recall: 0.9431 | F1: 0.9311\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train epoch 18: 100%|██████████| 939/939 [07:50<00:00,  2.00it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 18 - train avg loss: 0.0842 | IoU: 0.8967 | Dice: 0.9405 | Acc: 0.9831\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val loss: 0.1140 | IoU: 0.8845 | Dice: 0.9311 | Acc: 0.9798\n",
            "Precision: 0.9351 | Recall: 0.9355 | F1: 0.9335\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train epoch 19: 100%|██████████| 939/939 [07:45<00:00,  2.02it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 19 - train avg loss: 0.0844 | IoU: 0.8980 | Dice: 0.9416 | Acc: 0.9832\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val loss: 0.1313 | IoU: 0.8795 | Dice: 0.9285 | Acc: 0.9773\n",
            "Precision: 0.9211 | Recall: 0.9451 | F1: 0.9297\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train epoch 20: 100%|██████████| 939/939 [07:43<00:00,  2.03it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 20 - train avg loss: 0.0828 | IoU: 0.8987 | Dice: 0.9419 | Acc: 0.9834\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val loss: 0.1111 | IoU: 0.8861 | Dice: 0.9325 | Acc: 0.9797\n",
            "Precision: 0.9335 | Recall: 0.9392 | F1: 0.9348\n",
            "Saved training history to Drive (training_history_preprocessed_metrics.csv)\n",
            "Model saved to /content/drive/MyDrive/Data Mining/Project/Model/deeplabv3_mbv3_preprocessing.pth\n"
          ]
        }
      ],
      "source": [
        "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
        "IMAGENET_STD  = [0.229, 0.224, 0.225]\n",
        "\n",
        "def set_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "class PreprocessedSegDataset(Dataset):\n",
        "    def __init__(self, df, size=512, augment=False, seed: int = None):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.size = size\n",
        "        self.augment = augment\n",
        "        if seed is not None:\n",
        "            set_seed(seed)\n",
        "        #color jitter + flip + small rotate + random crop\n",
        "        self.color_jitter = T.ColorJitter(brightness=0.15, contrast=0.15, saturation=0.1, hue=0.02)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def _resize(self, img, mask):\n",
        "        img = img.resize((self.size, self.size), Image.BILINEAR)\n",
        "        mask = mask.resize((self.size, self.size), Image.NEAREST)\n",
        "        return img, mask\n",
        "\n",
        "    def _augment(self, img, mask):\n",
        "        # Horizontal flip\n",
        "        if random.random() < 0.5:\n",
        "            img = ImageOps.mirror(img)\n",
        "            mask = ImageOps.mirror(mask)\n",
        "\n",
        "        # small rotation: use InterpolationMode to avoid 'resample' kwarg error\n",
        "        if random.random() < 0.3:\n",
        "            angle = random.uniform(-12, 12)\n",
        "            img = TF.rotate(img, angle, interpolation=InterpolationMode.BILINEAR, expand=False)\n",
        "            mask = TF.rotate(mask, angle, interpolation=InterpolationMode.NEAREST, expand=False)\n",
        "\n",
        "        # random crop + resize (mild)\n",
        "        if random.random() < 0.25:\n",
        "            w, h = img.size\n",
        "            scale = random.uniform(0.88, 1.0)\n",
        "            new_w, new_h = int(w*scale), int(h*scale)\n",
        "            left = random.randint(0, max(0, w-new_w))\n",
        "            top  = random.randint(0, max(0, h-new_h))\n",
        "            img = img.crop((left, top, left+new_w, top+new_h))\n",
        "            mask = mask.crop((left, top, left+new_w, top+new_h))\n",
        "            img = img.resize((self.size, self.size), Image.BILINEAR)\n",
        "            mask = mask.resize((self.size, self.size), Image.NEAREST)\n",
        "\n",
        "        # color jitter (applied to PIL image)\n",
        "        img = self.color_jitter(img)\n",
        "\n",
        "        return img, mask\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.loc[idx]\n",
        "        img_path  = row[\"images\"]\n",
        "        mask_path = row[\"masks\"]\n",
        "        coef      = row.get(\"coef\", 1)\n",
        "\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        mask = Image.open(mask_path).convert(\"L\")\n",
        "\n",
        "        # deterministic resize first\n",
        "        img, mask = self._resize(img, mask)\n",
        "\n",
        "        # augment\n",
        "        if self.augment:\n",
        "            img, mask = self._augment(img, mask)\n",
        "\n",
        "        # --- Convert to numpy ---\n",
        "        img_np = np.array(img, dtype=np.float32)\n",
        "        mask_np = np.array(mask, dtype=np.float32)\n",
        "\n",
        "        # --- Robust mask normalization like Raw ---\n",
        "        max_val = mask_np.max() if mask_np.max() > 0 else 1.0\n",
        "        if max_val > 1.0:\n",
        "            mask_np = mask_np / max_val\n",
        "\n",
        "        mask_bin = (mask_np >= 0.5).astype(np.float32)\n",
        "\n",
        "        # image scaling\n",
        "        img_np = img_np / 255.0\n",
        "\n",
        "        # to tensor\n",
        "        img_t = torch.from_numpy(img_np).permute(2,0,1).float()\n",
        "        mask_t = torch.from_numpy(mask_bin).unsqueeze(0).float()\n",
        "\n",
        "        # ImageNet normalize\n",
        "        mean = torch.tensor(IMAGENET_MEAN, dtype=torch.float32).view(3,1,1)\n",
        "        std  = torch.tensor(IMAGENET_STD, dtype=torch.float32).view(3,1,1)\n",
        "        img_t = (img_t - mean) / std\n",
        "\n",
        "        return img_t, mask_t\n",
        "\n",
        "\n",
        "class ASPP(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, 1)\n",
        "        self.conv6 = nn.Conv2d(in_channels, out_channels, 3, padding=6, dilation=6)\n",
        "        self.conv12 = nn.Conv2d(in_channels, out_channels, 3, padding=12, dilation=12)\n",
        "        self.conv18 = nn.Conv2d(in_channels, out_channels, 3, padding=18, dilation=18)\n",
        "\n",
        "        self.global_pool = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Conv2d(in_channels, out_channels, 1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        self.out_conv = nn.Conv2d(out_channels * 5, out_channels, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h, w = x.shape[2], x.shape[3]\n",
        "        y1 = self.conv1(x)\n",
        "        y2 = self.conv6(x)\n",
        "        y3 = self.conv12(x)\n",
        "        y4 = self.conv18(x)\n",
        "        y5 = self.global_pool(x)\n",
        "        y5 = F.interpolate(y5, size=(h, w), mode=\"bilinear\", align_corners=False)\n",
        "        y = torch.cat([y1, y2, y3, y4, y5], dim=1)\n",
        "        return self.out_conv(y)\n",
        "\n",
        "class DeepLabV3_MobileNetV3Large(nn.Module):\n",
        "    def __init__(self, num_classes=1):\n",
        "        super().__init__()\n",
        "        base = mobilenet_v3_large(weights=MobileNet_V3_Large_Weights.DEFAULT)\n",
        "        self.backbone = base.features\n",
        "        backbone_out = 960\n",
        "        self.aspp = ASPP(backbone_out, 256)\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Conv2d(256, 128, 3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, num_classes, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.backbone(x)\n",
        "        x = self.aspp(x)\n",
        "        x = self.decoder(x)\n",
        "        x = F.interpolate(x, size=(512, 512), mode=\"bilinear\", align_corners=False)\n",
        "        return x\n",
        "\n",
        "train_ds = PreprocessedSegDataset(X_train_raw, size=512, augment=True, seed=42)\n",
        "val_ds   = PreprocessedSegDataset(X_test_raw,  size=512, augment=False, seed=42)\n",
        "history_name = \"training_history_preprocessed_metrics.csv\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = DeepLabV3_MobileNetV3Large(num_classes=1).to(device)\n",
        "\n",
        "criterion_bce = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=8, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=8, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "num_epochs = 20\n",
        "history = {\n",
        "    \"train_loss\": [],\n",
        "    \"val_loss\": [],\n",
        "    \"val_iou\": [],\n",
        "    \"val_dice\": [],\n",
        "    \"val_acc\": [],\n",
        "    \"val_prec\": [],\n",
        "    \"val_recall\": [],\n",
        "    \"val_f1\": []\n",
        "}\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    train_iou = 0.0\n",
        "    train_dice = 0.0\n",
        "    train_acc = 0.0\n",
        "    train_steps = 0\n",
        "\n",
        "    for imgs, masks in tqdm(train_loader, desc=f\"Train epoch {epoch+1}\"):\n",
        "        imgs = imgs.to(device).float()\n",
        "        masks = masks.to(device).float()\n",
        "\n",
        "        logits = model(imgs)\n",
        "        bce = criterion_bce(logits, masks)\n",
        "        dsc = dice_loss(logits, masks)\n",
        "        loss = bce + dsc\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            _, preds = logits_to_probs_preds(logits, thr=0.5)\n",
        "            train_iou += batch_iou(preds, masks)\n",
        "            train_dice += batch_dice(preds, masks)\n",
        "            train_acc += batch_pixel_accuracy(preds, masks)\n",
        "            train_steps += 1\n",
        "\n",
        "    avg_loss = running_loss / max(1, len(train_loader))\n",
        "    avg_train_iou = train_iou / max(1, train_steps)\n",
        "    avg_train_dice = train_dice / max(1, train_steps)\n",
        "    avg_train_acc = train_acc / max(1, train_steps)\n",
        "    print(f\"Epoch {epoch+1} - train avg loss: {avg_loss:.4f} | IoU: {avg_train_iou:.4f} | Dice: {avg_train_dice:.4f} | Acc: {avg_train_acc:.4f}\")\n",
        "    history[\"train_loss\"].append(avg_loss)\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_steps = 0\n",
        "\n",
        "    sum_iou = 0.0\n",
        "    sum_dice = 0.0\n",
        "    sum_acc = 0.0\n",
        "    sum_prec = 0.0\n",
        "    sum_recall = 0.0\n",
        "    sum_f1 = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, masks in val_loader:\n",
        "            imgs = imgs.to(device).float()\n",
        "            masks = masks.to(device).float()\n",
        "\n",
        "            logits = model(imgs)\n",
        "            bce = criterion_bce(logits, masks)\n",
        "            dsc = dice_loss(logits, masks)\n",
        "            batch_loss = (bce + dsc).item()\n",
        "            val_loss += batch_loss\n",
        "\n",
        "            probs, preds = logits_to_probs_preds(logits, thr=0.5)\n",
        "\n",
        "            # metrics\n",
        "            iou_b = batch_iou(preds, masks)\n",
        "            dice_b = batch_dice(preds, masks)\n",
        "            acc_b = batch_pixel_accuracy(preds, masks)\n",
        "            prec_b, recall_b, f1_b = batch_precision_recall_f1(preds, masks)\n",
        "\n",
        "            sum_iou += iou_b\n",
        "            sum_dice += dice_b\n",
        "            sum_acc += acc_b\n",
        "            sum_prec += prec_b\n",
        "            sum_recall += recall_b\n",
        "            sum_f1 += f1_b\n",
        "\n",
        "            val_steps += 1\n",
        "\n",
        "    avg_val_loss = val_loss / max(1, val_steps)\n",
        "    avg_val_iou = sum_iou / max(1, val_steps)\n",
        "    avg_val_dice = sum_dice / max(1, val_steps)\n",
        "    avg_val_acc = sum_acc / max(1, val_steps)\n",
        "    avg_val_prec = sum_prec / max(1, val_steps)\n",
        "    avg_val_recall = sum_recall / max(1, val_steps)\n",
        "    avg_val_f1 = sum_f1 / max(1, val_steps)\n",
        "\n",
        "    history[\"val_loss\"].append(avg_val_loss)\n",
        "    history[\"val_iou\"].append(avg_val_iou)\n",
        "    history[\"val_dice\"].append(avg_val_dice)\n",
        "    history[\"val_acc\"].append(avg_val_acc)\n",
        "    history[\"val_prec\"].append(avg_val_prec)\n",
        "    history[\"val_recall\"].append(avg_val_recall)\n",
        "    history[\"val_f1\"].append(avg_val_f1)\n",
        "\n",
        "    print(f\"Val loss: {avg_val_loss:.4f} | IoU: {avg_val_iou:.4f} | Dice: {avg_val_dice:.4f} | Acc: {avg_val_acc:.4f}\")\n",
        "    print(f\"Precision: {avg_val_prec:.4f} | Recall: {avg_val_recall:.4f} | F1: {avg_val_f1:.4f}\")\n",
        "\n",
        "# save history\n",
        "hist_df = pd.DataFrame(history)\n",
        "out_path = f\"/content/drive/MyDrive/Data Mining/Project/Model/{history_name}\"\n",
        "os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
        "hist_df.to_csv(out_path, index=False)\n",
        "print(f\"Saved training history to Drive ({history_name})\")\n",
        "save_path = \"/content/drive/MyDrive/Data Mining/Project/Model/deeplabv3_mbv3_preprocessing.pth\"\n",
        "\n",
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "}, save_path)\n",
        "\n",
        "print(f\"Model saved to {save_path}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}